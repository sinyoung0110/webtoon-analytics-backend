from fastapi import FastAPI, HTTPException, Query
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
import pandas as pd
import numpy as np
from typing import List, Dict, Optional
import json
import os
from datetime import datetime
import uvicorn
from dotenv import load_dotenv
from pathlib import Path
import re
from collections import defaultdict, Counter
from itertools import combinations
from sklearn.metrics.pairwise import cosine_similarity
import warnings
warnings.filterwarnings('ignore')

load_dotenv()

app = FastAPI(
    title="ì›¹íˆ° ë¶„ì„ API",
    description="ì›¹íˆ° ë°ì´í„° ë¶„ì„ ë° ì¶”ì²œ ì‹œìŠ¤í…œ API",
    version="1.0.0"
)

# CORS ì„¤ì •
app.add_middleware(
    CORSMiddleware,
    allow_origins=[
        "http://localhost:3000",
        "https://webtoon-analytics-dashboard.vercel.app",
        "https://webtoon-analytics-dashboard-1flmwo7bk.vercel.app",
    ],
    allow_credentials=True,
    allow_methods=["GET", "POST", "OPTIONS"],
    allow_headers=[
        "Accept",
        "Accept-Language",
        "Content-Language",
        "Content-Type",
        "Authorization",
    ],
)

# ë°ì´í„° ëª¨ë¸
class WebtoonData(BaseModel):
    rank: int
    title: str
    tags: List[str]
    interest_count: int
    rating: float
    gender: str
    ages: str

class RecommendationRequest(BaseModel):
    title: str
    limit: Optional[int] = 5

class NetworkAnalysisRequest(BaseModel):
    selected_tags: Optional[List[str]] = []
    min_cooccurrence: Optional[float] = 0.2
    max_nodes: Optional[int] = 30

class AnalysisResponse(BaseModel):
    success: bool
    data: Optional[Dict] = None
    message: Optional[str] = None

# íƒœê·¸ ì •ê·œí™” ë§¤í•‘ (í•œêµ­ì–´ ê¸°ë°˜)
TAG_NORMALIZATION = {
    # ë¡œë§¨ìŠ¤ ê´€ë ¨
    "ì™„ê²°ë¡œë§¨ìŠ¤": "ë¡œë§¨ìŠ¤",
    "ì™„ê²° ë¡œë§¨ìŠ¤": "ë¡œë§¨ìŠ¤", 
    "ìˆœì •": "ë¡œë§¨ìŠ¤",
    "ì—°ì• ": "ë¡œë§¨ìŠ¤",
    "ëŸ¬ë¸Œ": "ë¡œë§¨ìŠ¤",
    "ìˆœì •ë‚¨": "ë¡œë§¨ìŠ¤",
    "ì²«ì‚¬ë‘": "ë¡œë§¨ìŠ¤",
    "ì†Œê¿‰ì¹œêµ¬": "ë¡œë§¨ìŠ¤",
    "ë¦¬ì–¼ë¡œë§¨ìŠ¤":"ë¡œë§¨ìŠ¤",
    
    # ì•¡ì…˜ ê´€ë ¨
    "ì™„ê²°ì•¡ì…˜": "ì•¡ì…˜",
    "ì™„ê²° ì•¡ì…˜": "ì•¡ì…˜",
    "ë°°í‹€": "ì•¡ì…˜",
    "ê²©íˆ¬": "ì•¡ì…˜",
    "ì „íˆ¬": "ì•¡ì…˜",
    "ê²©íˆ¬ê¸°": "ì•¡ì…˜",
    "í•™ì›ì•¡ì…˜": "ì•¡ì…˜",
    
    # íŒíƒ€ì§€ ê´€ë ¨
    "ì™„ê²°íŒíƒ€ì§€": "íŒíƒ€ì§€",
    "ì™„ê²° íŒíƒ€ì§€": "íŒíƒ€ì§€",
    "ë§ˆë²•": "íŒíƒ€ì§€",
    "í™˜ìƒ": "íŒíƒ€ì§€",
    "ì´ì„¸ê³„": "íŒíƒ€ì§€",
    "ì´ëŠ¥ë ¥": "íŒíƒ€ì§€",
    
    # ë“œë¼ë§ˆ ê´€ë ¨
    "ì™„ê²°ë“œë¼ë§ˆ": "ë“œë¼ë§ˆ",
    "ì™„ê²° ë“œë¼ë§ˆ": "ë“œë¼ë§ˆ",
    "ë©œë¡œ": "ë“œë¼ë§ˆ",
    "ê°ë™": "ë“œë¼ë§ˆ",
    "ê°ì„±ë“œë¼ë§ˆ": "ë“œë¼ë§ˆ",
    "ê°ì„±ì ì¸": "ë“œë¼ë§ˆ",
    
    # ìŠ¤ë¦´ëŸ¬ ê´€ë ¨
    "ì™„ê²°ìŠ¤ë¦´ëŸ¬": "ìŠ¤ë¦´ëŸ¬",
    "ì™„ê²° ìŠ¤ë¦´ëŸ¬": "ìŠ¤ë¦´ëŸ¬",
    "ì„œìŠ¤íœìŠ¤": "ìŠ¤ë¦´ëŸ¬",
    "ë¯¸ìŠ¤í„°ë¦¬": "ìŠ¤ë¦´ëŸ¬",
    
    # ì¼ìƒ ê´€ë ¨
    "ì™„ê²°ì¼ìƒ": "ì¼ìƒ",
    "ì™„ê²° ì¼ìƒ": "ì¼ìƒ",
    "íë§": "ì¼ìƒ",
    "ì†Œì†Œí•œ": "ì¼ìƒ",
    
    # ì„±ì¥/ë¬´í˜‘ ê´€ë ¨
    "ì„±ì¥ë¬¼": "ì„±ì¥",
    "ë ˆë²¨ì—…": "ì„±ì¥",
    "ë¬´í˜‘/ì‚¬ê·¹": "ë¬´í˜‘",
    "ì‚¬ê·¹": "ë¬´í˜‘",
    
    # ê¸°íƒ€
    "ì™•ì¡±/ê·€ì¡±": "ê·€ì¡±",
    "ê°œê·¸": "ì½”ë¯¸ë””",
    "ëŸ¬ë¸”ë¦¬": "ì¼ìƒ",
    "ëª…ì‘": "ëª…ì‘",
}

def normalize_tag(tag):
    """íƒœê·¸ ì •ê·œí™”"""
    if not tag:
        return tag
    
    tag = tag.strip()
    normalized = TAG_NORMALIZATION.get(tag, tag)
    return normalized

def normalize_tags_in_data(webtoons_data):
    """ë°ì´í„°ì˜ ëª¨ë“  íƒœê·¸ ì •ê·œí™”"""
    for webtoon in webtoons_data:
        webtoon['tags'] = [normalize_tag(tag) for tag in webtoon['tags']]
        webtoon['normalized_tags'] = list(set(webtoon['tags']))  # ì¤‘ë³µ ì œê±°
    return webtoons_data

# ìƒ˜í”Œ ë°ì´í„° (í•œêµ­ì–´ íƒœê·¸ë¡œ ì—…ë°ì´íŠ¸)
SAMPLE_WEBTOONS = [
    {"rank": 1, "title": "í™”ì‚°ê·€í™˜", "tags": ["íšŒê·€", "ë¬´í˜‘", "ì•¡ì…˜", "ëª…ì‘"], "interest_count": 1534623, "rating": 9.88, "gender": "ë‚¨ì„±", "ages": "20ëŒ€"},
    {"rank": 2, "title": "ì‹ ì˜ íƒ‘", "tags": ["íŒíƒ€ì§€", "ì•¡ì…˜", "ì„±ì¥"], "interest_count": 1910544, "rating": 9.84, "gender": "ë‚¨ì„±", "ages": "20ëŒ€"},
    {"rank": 3, "title": "ì™¸ëª¨ì§€ìƒì£¼ì˜", "tags": ["ë“œë¼ë§ˆ", "í•™ì›", "ì•¡ì…˜"], "interest_count": 824399, "rating": 9.40, "gender": "ë‚¨ì„±", "ages": "10ëŒ€"},
    {"rank": 4, "title": "ë§ˆë¥¸ ê°€ì§€ì— ë°”ëŒì²˜ëŸ¼", "tags": ["ë¡œë§¨ìŠ¤", "ê·€ì¡±", "ì„œì–‘"], "interest_count": 458809, "rating": 9.97, "gender": "ì—¬ì„±", "ages": "10ëŒ€"},
    {"rank": 5, "title": "ì—„ë§ˆë¥¼ ë§Œë‚˜ëŸ¬ ê°€ëŠ” ê¸¸", "tags": ["íŒíƒ€ì§€", "ëª¨í—˜", "ì¼ìƒ"], "interest_count": 259146, "rating": 9.98, "gender": "ì—¬ì„±", "ages": "10ëŒ€"},
    {"rank": 6, "title": "ì¬í˜¼ í™©í›„", "tags": ["ë¡œë§¨ìŠ¤", "ê·€ì¡±", "ì„œì–‘", "ë³µìˆ˜"], "interest_count": 892456, "rating": 9.75, "gender": "ì—¬ì„±", "ages": "20ëŒ€"},
    {"rank": 7, "title": "ë‚˜ í˜¼ìë§Œ ë ˆë²¨ì—…", "tags": ["ì•¡ì…˜", "ê²Œì„", "íŒíƒ€ì§€", "ì„±ì¥"], "interest_count": 2156789, "rating": 9.91, "gender": "ë‚¨ì„±", "ages": "20ëŒ€"},
    {"rank": 8, "title": "ì—¬ì‹ ê°•ë¦¼", "tags": ["ë¡œë§¨ìŠ¤", "í•™ì›", "ì¼ìƒ", "ì½”ë¯¸ë””"], "interest_count": 1345678, "rating": 9.62, "gender": "ì—¬ì„±", "ages": "10ëŒ€"},
    {"rank": 9, "title": "ì´íƒœì› í´ë¼ì“°", "tags": ["ë“œë¼ë§ˆ", "í˜„ì‹¤", "ì„±ì¥"], "interest_count": 987654, "rating": 9.55, "gender": "ë‚¨ì„±", "ages": "30ëŒ€"},
    {"rank": 10, "title": "ìœ ë¯¸ì˜ ì„¸í¬ë“¤", "tags": ["ë¡œë§¨ìŠ¤", "ì¼ìƒ", "ë“œë¼ë§ˆ"], "interest_count": 756432, "rating": 9.33, "gender": "ì—¬ì„±", "ages": "30ëŒ€"},
    {"rank": 11, "title": "ì „ì§€ì  ë…ì ì‹œì ", "tags": ["íšŒê·€", "íŒíƒ€ì§€", "ì•¡ì…˜", "ì„±ì¥"], "interest_count": 1823456, "rating": 9.92, "gender": "ë‚¨ì„±", "ages": "20ëŒ€"},
    {"rank": 12, "title": "ì•…ì—­ì˜ ì—”ë”©ì€ ì£½ìŒë¿", "tags": ["ë¡œë§¨ìŠ¤", "íšŒê·€", "íŒíƒ€ì§€", "ê·€ì¡±"], "interest_count": 734521, "rating": 9.78, "gender": "ì—¬ì„±", "ages": "20ëŒ€"},
    {"rank": 13, "title": "ë‚˜ì˜ ìˆ˜í•™ì„ ìƒ", "tags": ["ë¡œë§¨ìŠ¤", "í•™ì›", "ë“œë¼ë§ˆ", "ì¼ìƒ"], "interest_count": 654321, "rating": 9.45, "gender": "ì—¬ì„±", "ages": "20ëŒ€"},
    {"rank": 14, "title": "ëŒ€í•™ì› íƒˆì¶œì¼ì§€", "tags": ["ì¼ìƒ", "ì½”ë¯¸ë””", "í˜„ì‹¤"], "interest_count": 543210, "rating": 9.23, "gender": "ë‚¨ì„±", "ages": "20ëŒ€"},
    {"rank": 15, "title": "ê¸°ê¸°ê´´ê´´", "tags": ["ìŠ¤ë¦´ëŸ¬", "í˜¸ëŸ¬", "ë‹¨í¸"], "interest_count": 432109, "rating": 9.34, "gender": "ë‚¨ì„±", "ages": "20ëŒ€"},
    {"rank": 16, "title": "ìœˆë“œë¸Œë ˆì´ì»¤", "tags": ["ì•¡ì…˜", "í•™ì›", "ì„±ì¥"], "interest_count": 687234, "rating": 9.67, "gender": "ë‚¨ì„±", "ages": "10ëŒ€"},
    {"rank": 17, "title": "ì°¸êµìœ¡", "tags": ["ì•¡ì…˜", "í•™ì›", "í˜„ì‹¤"], "interest_count": 923145, "rating": 9.12, "gender": "ë‚¨ì„±", "ages": "20ëŒ€"},
    {"rank": 18, "title": "í•˜ë£¨ë§Œ ë„¤ê°€ ë˜ê³  ì‹¶ì–´", "tags": ["ë¡œë§¨ìŠ¤", "ì¼ìƒ", "ë“œë¼ë§ˆ"], "interest_count": 445678, "rating": 9.56, "gender": "ì—¬ì„±", "ages": "10ëŒ€"},
    {"rank": 19, "title": "ì·¨ì‚¬ë³‘", "tags": ["ìš”ë¦¬", "ì¼ìƒ", "ë“œë¼ë§ˆ"], "interest_count": 534219, "rating": 9.23, "gender": "ë‚¨ì„±", "ages": "30ëŒ€"},
    {"rank": 20, "title": "í”„ë¦¬ë“œë¡œìš°", "tags": ["ë†êµ¬", "ìŠ¤í¬ì¸ ", "ì„±ì¥"], "interest_count": 678912, "rating": 9.45, "gender": "ë‚¨ì„±", "ages": "20ëŒ€"},
]

def parse_tags(tags_str):
    """íƒœê·¸ ë¬¸ìì—´ì„ ë¦¬ìŠ¤íŠ¸ë¡œ ì•ˆì „í•˜ê²Œ íŒŒì‹±"""
    if pd.isna(tags_str):
        return []
    
    tags_str = str(tags_str).strip()
    
    if tags_str.startswith('[') and tags_str.endswith(']'):
        try:
            import ast
            return [normalize_tag(tag) for tag in ast.literal_eval(tags_str)]
        except:
            tags_str = tags_str[1:-1]
    
    tags = []
    for tag in tags_str.split(','):
        tag = tag.strip().strip("'\"")
        if tag:
            tags.append(normalize_tag(tag))
    
    return tags

def load_webtoon_data_from_csv_safe():
    """ì•ˆì „í•œ CSV ë°ì´í„° ë¡œë“œ"""
    try:
        csv_path = Path(__file__).parent / "final_webtoon_clean.csv"
        
        if not csv_path.exists():
            print(f"CSV íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {csv_path}")
            return normalize_tags_in_data(SAMPLE_WEBTOONS)
        
        df = pd.read_csv(csv_path)
        print(f"CSV íŒŒì¼ì—ì„œ {len(df)}ê°œ í–‰ì„ ì½ì—ˆìŠµë‹ˆë‹¤.")
        
        webtoons_data = []
        for idx, row in df.iterrows():
            try:
                webtoon = {
                    "rank": int(row['rank']),
                    "title": str(row['title']),
                    "summary": str(row.get('summary', '')),
                    "tags": parse_tags(row['tags']),
                    "interest_count": int(row['interest_count']),
                    "rating": float(row['rating']),
                    "gender": str(row['gender']),
                    "ages": str(row['ages'])
                }
                webtoons_data.append(webtoon)
            except Exception as e:
                print(f"í–‰ {idx} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                continue
        
        print(f"ì„±ê³µì ìœ¼ë¡œ {len(webtoons_data)}ê°œì˜ ì›¹íˆ° ë°ì´í„°ë¥¼ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
        return normalize_tags_in_data(webtoons_data)
        
    except Exception as e:
        print(f"CSV ë¡œë”© ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return normalize_tags_in_data(SAMPLE_WEBTOONS)

def load_webtoon_data():
    """ì›¹íˆ° ë°ì´í„° ë¡œë“œ"""
    return load_webtoon_data_from_csv_safe()

def create_tag_matrix(webtoons_data, min_count=3):
    """íƒœê·¸ ë™ì‹œ ì¶œí˜„ ë§¤íŠ¸ë¦­ìŠ¤ ìƒì„± (Python ë¶„ì„ ì½”ë“œ ê¸°ë°˜)"""
    print("ğŸ·ï¸ íƒœê·¸ ë™ì‹œ ì¶œí˜„ ë§¤íŠ¸ë¦­ìŠ¤ ìƒì„± ì¤‘...")
    
    # ëª¨ë“  íƒœê·¸ ìˆ˜ì§‘ ë° ë¹ˆë„ ê³„ì‚°
    all_tags = []
    for webtoon in webtoons_data:
        if isinstance(webtoon['tags'], list):
            all_tags.extend(webtoon['tags'])
    
    tag_counts = Counter(all_tags)
    
    # ìµœì†Œ ë¹ˆë„ ì´ìƒì¸ íƒœê·¸ë§Œ ì„ íƒ
    frequent_tags = [tag for tag, count in tag_counts.items() if count >= min_count]
    frequent_tags = sorted(frequent_tags, key=lambda x: tag_counts[x], reverse=True)
    
    print(f"ğŸ“Š ì´ ê³ ìœ  íƒœê·¸: {len(tag_counts)}ê°œ")
    print(f"ğŸ“Š ë¶„ì„ ëŒ€ìƒ íƒœê·¸ ({min_count}íšŒ ì´ìƒ): {len(frequent_tags)}ê°œ")
    
    # íƒœê·¸ ë™ì‹œ ì¶œí˜„ ë§¤íŠ¸ë¦­ìŠ¤ ìƒì„±
    tag_matrix = np.zeros((len(frequent_tags), len(frequent_tags)))
    tag_to_idx = {tag: idx for idx, tag in enumerate(frequent_tags)}
    
    for webtoon in webtoons_data:
        current_tags = [tag for tag in webtoon['tags'] if tag in tag_to_idx]
        
        # ë™ì‹œ ì¶œí˜„ ê¸°ë¡ (ê°€ì¤‘ì¹˜ ì ìš©)
        weight = (webtoon['rating'] / 10.0) * (1 + np.log10(webtoon['interest_count'] + 1) / 10)
        
        for tag in current_tags:
            tag_matrix[tag_to_idx[tag], tag_to_idx[tag]] += weight
        
        for tag1, tag2 in combinations(current_tags, 2):
            idx1, idx2 = tag_to_idx[tag1], tag_to_idx[tag2]
            tag_matrix[idx1, idx2] += weight
            tag_matrix[idx2, idx1] += weight
    
    return tag_matrix, frequent_tags, tag_counts

def calculate_tag_correlations(tag_matrix, frequent_tags):
    """íƒœê·¸ ê°„ ìƒê´€ê³„ìˆ˜ ê³„ì‚° (Python ë¶„ì„ ì½”ë“œ ê¸°ë°˜)"""
    print("ğŸ”— íƒœê·¸ ìƒê´€ê´€ê³„ ê³„ì‚° ì¤‘...")
    
    # ëŒ€ê°ì„  ìš”ì†Œë¥¼ 0ìœ¼ë¡œ ì„¤ì •
    np.fill_diagonal(tag_matrix, 0)
    
    # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
    correlation_matrix = cosine_similarity(tag_matrix)
    
    # ìƒê´€ê´€ê³„ê°€ ë†’ì€ íƒœê·¸ ìŒ ì°¾ê¸°
    correlations = []
    n_tags = len(frequent_tags)
    
    for i in range(n_tags):
        for j in range(i+1, n_tags):
            if correlation_matrix[i, j] > 0:
                correlations.append({
                    'tag1': frequent_tags[i],
                    'tag2': frequent_tags[j],
                    'correlation': correlation_matrix[i, j],
                    'co_occurrence': tag_matrix[i, j]
                })
    
    correlations = sorted(correlations, key=lambda x: x['correlation'], reverse=True)
    return correlation_matrix, correlations

def create_advanced_network_data(webtoons_data, selected_tags=None, min_correlation=0.2, max_nodes=30):
    """ê³ ê¸‰ ë„¤íŠ¸ì›Œí¬ ë°ì´í„° ìƒì„± (í•œêµ­ì–´ ê¸°ë°˜)"""
    print("ğŸ•¸ï¸ ê³ ê¸‰ ë„¤íŠ¸ì›Œí¬ ê·¸ë˜í”„ ìƒì„± ì¤‘...")
    
    # íƒœê·¸ ë§¤íŠ¸ë¦­ìŠ¤ ë° ìƒê´€ê´€ê³„ ê³„ì‚°
    tag_matrix, frequent_tags, tag_counts = create_tag_matrix(webtoons_data)
    _, correlations = calculate_tag_correlations(tag_matrix, frequent_tags)
    
    # ì „ì²´ íƒœê·¸ ì˜í–¥ë ¥ ê³„ì‚° (ëª¨ë“  ê²½ìš°ì— í•„ìš”)
    tag_influence = defaultdict(float)
    for tag in frequent_tags[:50]:
        frequency_score = tag_counts.get(tag, 0) / max(tag_counts.values())
        
        connection_score = 0
        for corr in correlations[:50]:
            if corr['tag1'] == tag or corr['tag2'] == tag:
                connection_score += corr['correlation']
        
        tag_influence[tag] = frequency_score * 0.6 + (connection_score / 10) * 0.4

    # ì„ íƒëœ íƒœê·¸ê°€ ìˆìœ¼ë©´ í•´ë‹¹ íƒœê·¸ì™€ ì—°ê²°ëœ ë…¸ë“œë“¤ë§Œ í¬í•¨
    if selected_tags:
        relevant_tags = set(selected_tags)
        for corr in correlations:
            if corr['correlation'] >= min_correlation:
                if corr['tag1'] in selected_tags or corr['tag2'] in selected_tags:
                    relevant_tags.add(corr['tag1'])
                    relevant_tags.add(corr['tag2'])
        
        # ì—°ê²°ì„±ì´ ë†’ì€ íƒœê·¸ë“¤ ìš°ì„  ì„ íƒ
        tag_connections = defaultdict(float)
        for corr in correlations[:100]:
            if corr['correlation'] >= min_correlation:
                tag_connections[corr['tag1']] += corr['correlation']
                tag_connections[corr['tag2']] += corr['correlation']
        
        sorted_tags = sorted(
            [(tag, score) for tag, score in tag_connections.items() if tag in relevant_tags],
            key=lambda x: x[1], reverse=True
        )
        top_tags = set([tag for tag, _ in sorted_tags[:max_nodes]])
    else:
        # ì „ì²´ íƒœê·¸ì—ì„œ ìƒìœ„ ë…¸ë“œ ì„ íƒ
        sorted_tags = sorted(tag_influence.items(), key=lambda x: x[1], reverse=True)
        top_tags = set([tag for tag, _ in sorted_tags[:max_nodes]])
    
    # ë…¸ë“œ ìƒì„± (í•œêµ­ì–´ ì¹´í…Œê³ ë¦¬)
    nodes = []
    for tag in top_tags:
        count = tag_counts.get(tag, 0)
        
        # íƒœê·¸ë³„ ì›¹íˆ°ì˜ í‰ê·  í‰ì ê³¼ ì¡°íšŒìˆ˜ ê³„ì‚°
        tag_webtoons = [w for w in webtoons_data if tag in w['tags']]
        avg_rating = np.mean([w['rating'] for w in tag_webtoons]) if tag_webtoons else 0
        avg_interest = np.mean([w['interest_count'] for w in tag_webtoons]) if tag_webtoons else 0
        
        # ì˜í–¥ë ¥ ì ìˆ˜ ê³„ì‚°
        influence = tag_influence.get(tag, 0)
        
        nodes.append({
            'id': tag,
            'count': count,
            'influence': round(influence, 3),
            'avg_rating': round(avg_rating, 2),
            'avg_interest': int(avg_interest),
            'size': min(max(count * 2, 15), 60),
            'group': get_korean_tag_category(tag),
            'selected': tag in (selected_tags or [])
        })
    
    # ë§í¬ ìƒì„±
    links = []
    node_ids = set(node['id'] for node in nodes)
    
    for corr in correlations:
        if (corr['correlation'] >= min_correlation and 
            corr['tag1'] in node_ids and corr['tag2'] in node_ids):
            
            links.append({
                'source': corr['tag1'],
                'target': corr['tag2'],
                'value': round(corr['correlation'], 3),
                'co_occurrence': round(corr['co_occurrence'], 1),
                'width': min(max(corr['correlation'] * 10, 1), 8)
            })
    
    # ìƒê´€ê´€ê³„ ìˆœìœ¼ë¡œ ì •ë ¬
    links.sort(key=lambda x: x['value'], reverse=True)
    
    return {
        'nodes': nodes,
        'links': links[:100],  # ìƒìœ„ 100ê°œ ë§í¬ë§Œ
        'summary': {
            'total_nodes': len(nodes),
            'total_links': len(links),
            'selected_tags': selected_tags or [],
            'max_correlation': max([l['value'] for l in links]) if links else 0,
            'avg_correlation': np.mean([l['value'] for l in links]) if links else 0
        },
        'analysis_stats': {
            'total_unique_tags': len(tag_counts),
            'frequent_tags_count': len(frequent_tags),
            'correlation_threshold': min_correlation
        }
    }

def get_korean_tag_category(tag):
    """í•œêµ­ì–´ íƒœê·¸ ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜"""
    categories = {
        'ì¥ë¥´': ['ë¡œë§¨ìŠ¤', 'ì•¡ì…˜', 'íŒíƒ€ì§€', 'ë“œë¼ë§ˆ', 'ìŠ¤ë¦´ëŸ¬', 'í˜¸ëŸ¬', 'ì½”ë¯¸ë””', 'ì¼ìƒ', 'ë¬´í˜‘'],
        'í…Œë§ˆ': ['íšŒê·€', 'ì„±ì¥', 'ë³µìˆ˜', 'í•™ì›', 'í˜„ì‹¤', 'ê²Œì„', 'ëª¨í—˜', 'ìš”ë¦¬', 'ìŠ¤í¬ì¸ '],
        'ìŠ¤íƒ€ì¼': ['ëª…ì‘', 'ë‹¨í¸', 'ëŸ¬ë¸”ë¦¬'],
        'ì„¤ì •': ['ì„œì–‘', 'ê·€ì¡±', 'í˜„ëŒ€', 'ë¯¸ë˜', 'ê³¼ê±°', 'ë†êµ¬']
    }
    
    for category, tags in categories.items():
        if any(keyword in tag for keyword in tags):
            return category
    
    return 'ê¸°íƒ€'

def get_related_tags_advanced(target_tag, webtoons_data, limit=10):
    """íŠ¹ì • íƒœê·¸ì™€ ê´€ë ¨ëœ íƒœê·¸ë“¤ ì°¾ê¸° (ê³ ê¸‰ ë²„ì „)"""
    print(f"ğŸ” '{target_tag}' íƒœê·¸ ê´€ë ¨ì„± ë¶„ì„ ì¤‘...")
    
    related_scores = defaultdict(float)
    target_webtoons = [w for w in webtoons_data if target_tag in w['tags']]
    
    for webtoon in target_webtoons:
        # ê°€ì¤‘ì¹˜: í‰ì ê³¼ ì¡°íšŒìˆ˜ë¥¼ ê³ ë ¤
        weight = (webtoon['rating'] / 10.0) * (1 + np.log10(webtoon['interest_count'] + 1) / 20)
        
        for tag in webtoon['tags']:
            if tag != target_tag:
                related_scores[tag] += weight
    
    # ì •ê·œí™”
    if related_scores:
        max_score = max(related_scores.values())
        for tag in related_scores:
            related_scores[tag] = related_scores[tag] / max_score
    
    sorted_related = sorted(related_scores.items(), key=lambda x: x[1], reverse=True)
    
    return [{
        'tag': tag,
        'score': round(score, 3),
        'count': sum(1 for w in target_webtoons if tag in w['tags']),
        'category': get_korean_tag_category(tag)
    } for tag, score in sorted_related[:limit]]

# API ì—”ë“œí¬ì¸íŠ¸ë“¤

@app.get("/")
async def read_root():
    return {
        "message": "ì›¹íˆ° ë¶„ì„ API ì„œë²„ê°€ ì •ìƒ ì‘ë™ ì¤‘ì…ë‹ˆë‹¤",
        "version": "1.0.0",
        "features": ["í•œêµ­ì–´ íƒœê·¸ ë„¤íŠ¸ì›Œí¬", "ê³ ê¸‰ ìƒê´€ê´€ê³„ ë¶„ì„", "ê°œì„ ëœ ì¶”ì²œ ì‹œìŠ¤í…œ"],
        "endpoints": {
            "webtoons": "/api/webtoons",
            "tag_analysis": "/api/analysis/tags",
            "network_analysis": "/api/analysis/network",
            "tag_connectivity": "/api/analysis/tag-connectivity",
            "related_tags": "/api/analysis/related-tags/{tag}",
            "heatmap": "/api/analysis/heatmap",
            "recommendations": "/api/recommendations",
            "statistics": "/api/stats"
        }
    }

@app.get("/api/webtoons")
async def get_webtoons():
    """ëª¨ë“  ì›¹íˆ° ë°ì´í„° ë°˜í™˜"""
    try:
        data = load_webtoon_data()
        return {"success": True, "data": data, "count": len(data)}
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ë°ì´í„° ë¡œë”© ì‹¤íŒ¨: {str(e)}")

@app.get("/api/analysis/tags")
async def get_tag_analysis():
    """íƒœê·¸ ë¶„ì„ ë°ì´í„° ë°˜í™˜ (í•œêµ­ì–´ ê¸°ë°˜)"""
    try:
        webtoons_data = load_webtoon_data()
        
        # ëª¨ë“  íƒœê·¸ ìˆ˜ì§‘
        all_tags = []
        for webtoon in webtoons_data:
            all_tags.extend(webtoon['tags'])
        
        tag_frequency = Counter(all_tags).most_common(20)
        
        return {
            "success": True,
            "data": {
                "tag_frequency": tag_frequency,
                "total_tags": len(set(all_tags)),
                "normalization_applied": True,
                "korean_support": True
            }
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"íƒœê·¸ ë¶„ì„ ì‹¤íŒ¨: {str(e)}")

@app.get("/api/analysis/network")
async def get_network_analysis(
    selected_tags: Optional[str] = Query(None, description="ì‰¼í‘œë¡œ êµ¬ë¶„ëœ ì„ íƒëœ íƒœê·¸ë“¤"),
    min_correlation: Optional[float] = Query(0.2, description="ìµœì†Œ ìƒê´€ê³„ìˆ˜"),
    max_nodes: Optional[int] = Query(30, description="ìµœëŒ€ ë…¸ë“œ ìˆ˜")
):
    """ê³ ê¸‰ í‚¤ì›Œë“œ ë„¤íŠ¸ì›Œí¬ ë¶„ì„ ë°ì´í„° ë°˜í™˜"""
    try:
        webtoons_data = load_webtoon_data()
        
        # ì„ íƒëœ íƒœê·¸ íŒŒì‹±
        selected_tag_list = []
        if selected_tags:
            selected_tag_list = [tag.strip() for tag in selected_tags.split(',') if tag.strip()]
        
        network_data = create_advanced_network_data(
            webtoons_data, 
            selected_tag_list, 
            min_correlation, 
            max_nodes
        )
        
        return {"success": True, "data": network_data}
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ë„¤íŠ¸ì›Œí¬ ë¶„ì„ ì‹¤íŒ¨: {str(e)}")

@app.get("/api/analysis/related-tags/{tag}")
async def get_related_tags_analysis(tag: str, limit: Optional[int] = Query(10)):
    """íŠ¹ì • íƒœê·¸ì™€ ê´€ë ¨ëœ íƒœê·¸ë“¤ ë°˜í™˜ (ê³ ê¸‰ ë¶„ì„)"""
    try:
        webtoons_data = load_webtoon_data()
        
        # íƒœê·¸ ì •ê·œí™”
        normalized_tag = normalize_tag(tag)
        
        related_tags = get_related_tags_advanced(normalized_tag, webtoons_data, limit)
        
        return {
            "success": True,
            "data": {
                "target_tag": normalized_tag,
                "related_tags": related_tags,
                "count": len(related_tags),
                "analysis_method": "weighted_correlation"
            }
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ê´€ë ¨ íƒœê·¸ ë¶„ì„ ì‹¤íŒ¨: {str(e)}")

@app.post("/api/recommendations")
async def get_recommendations(request: RecommendationRequest):
    """ê°œì„ ëœ ì¶”ì²œ ì›¹íˆ° ë°˜í™˜"""
    try:
        webtoons_data = load_webtoon_data()
        
        target_webtoon = next((w for w in webtoons_data if w['title'] == request.title), None)
        if not target_webtoon:
            return {"success": False, "message": "ì›¹íˆ°ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."}
        
        target_tags = set(target_webtoon['tags'])
        recommendations = []
        
        for webtoon in webtoons_data:
            if webtoon['title'] == request.title:
                continue
                
            webtoon_tags = set(webtoon['tags'])
            
            # ê°œì„ ëœ ìœ ì‚¬ë„ ê³„ì‚° (Jaccard + ê°€ì¤‘ì¹˜)
            intersection = len(target_tags & webtoon_tags)
            union = len(target_tags | webtoon_tags)
            jaccard_similarity = intersection / union if union > 0 else 0
            
            # í‰ì ê³¼ ì¡°íšŒìˆ˜ë¥¼ ê³ ë ¤í•œ ê°€ì¤‘ì¹˜
            rating_weight = webtoon['rating'] / 10.0
            popularity_weight = min(webtoon['interest_count'] / 1000000, 1.0)
            
            final_similarity = jaccard_similarity * 0.7 + rating_weight * 0.2 + popularity_weight * 0.1
            
            if final_similarity > 0:
                recommendations.append({
                    **webtoon,
                    'similarity': round(final_similarity, 3),
                    'jaccard_similarity': round(jaccard_similarity, 3),
                    'common_tags': list(target_tags & webtoon_tags),
                    'reason': f"ê³µí†µ íƒœê·¸ {intersection}ê°œ, ìœ ì‚¬ë„ {final_similarity:.1%}"
                })
        
        recommendations.sort(key=lambda x: x['similarity'], reverse=True)
        
        return {
            "success": True, 
            "data": recommendations[:request.limit],
            "count": len(recommendations),
            "requested_title": request.title,
            "target_tags": list(target_tags),
            "algorithm": "enhanced_weighted_similarity"
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ì¶”ì²œ ìƒì„± ì‹¤íŒ¨: {str(e)}")

@app.get("/api/stats")
async def get_statistics():
    """ì „ì²´ í†µê³„ ë°˜í™˜ (í•œêµ­ì–´ ê°œì„ )"""
    try:
        webtoons_data = load_webtoon_data()
        
        total_webtoons = len(webtoons_data)
        avg_rating = np.mean([w['rating'] for w in webtoons_data])
        avg_interest = np.mean([w['interest_count'] for w in webtoons_data])
        
        # ì •ê·œí™”ëœ íƒœê·¸ í†µê³„
        all_tags = []
        for w in webtoons_data:
            all_tags.extend(w['tags'])
        unique_tags = len(set(all_tags))
        tag_frequency = Counter(all_tags)
        
        # ì„±ë³„/ì—°ë ¹ë³„ ë¶„í¬
        gender_dist = defaultdict(int)
        age_dist = defaultdict(int)
        for w in webtoons_data:
            gender_dist[w['gender']] += 1
            age_dist[w['ages']] += 1
        
        # ì¥ë¥´ë³„ ë¶„í¬
        genre_dist = defaultdict(int)
        main_genres = ['ë¡œë§¨ìŠ¤', 'ì•¡ì…˜', 'íŒíƒ€ì§€', 'ë“œë¼ë§ˆ', 'ìŠ¤ë¦´ëŸ¬', 'ì¼ìƒ', 'ë¬´í˜‘']
        for genre in main_genres:
            genre_dist[genre] = sum(1 for w in webtoons_data if genre in w['tags'])
        
        return {
            "success": True,
            "data": {
                "total_webtoons": total_webtoons,
                "avg_rating": round(avg_rating, 2),
                "avg_interest": int(avg_interest),
                "unique_tags": unique_tags,
                "gender_distribution": dict(gender_dist),
                "age_distribution": dict(age_dist),
                "genre_distribution": dict(genre_dist),
                "top_tags": tag_frequency.most_common(10),
                "normalization_stats": {
                    "normalized_mappings": len(TAG_NORMALIZATION),
                    "korean_tags_supported": True,
                    "example_normalizations": dict(list(TAG_NORMALIZATION.items())[:5])
                },
                "last_updated": datetime.now().isoformat()
            }
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"í†µê³„ ê³„ì‚° ì‹¤íŒ¨: {str(e)}")

def generate_heatmap_data(webtoons_data):
    """íˆíŠ¸ë§µ ë°ì´í„° ìƒì„± (í•œêµ­ì–´ ì¥ë¥´)"""
    genres = ['ë¡œë§¨ìŠ¤', 'ì•¡ì…˜', 'íŒíƒ€ì§€', 'ë“œë¼ë§ˆ', 'ë¬´í˜‘', 'ì¼ìƒ', 'ìŠ¤ë¦´ëŸ¬']
    demographics = ['ë‚¨ì„±-10ëŒ€', 'ë‚¨ì„±-20ëŒ€', 'ë‚¨ì„±-30ëŒ€', 'ì—¬ì„±-10ëŒ€', 'ì—¬ì„±-20ëŒ€', 'ì—¬ì„±-30ëŒ€']
    
    heatmap_data = []
    
    for demo_idx, demo in enumerate(demographics):
        gender, age = demo.split('-')
        for genre_idx, genre in enumerate(genres):
            count = sum(1 for w in webtoons_data 
                       if w['gender'] == gender and w['ages'] == age and genre in w['tags'])
            
            # í‰ê·  í‰ì ë„ ê³„ì‚°
            genre_webtoons = [w for w in webtoons_data 
                            if w['gender'] == gender and w['ages'] == age and genre in w['tags']]
            avg_rating = np.mean([w['rating'] for w in genre_webtoons]) if genre_webtoons else 0
            
            heatmap_data.append({
                'x': genre_idx,
                'y': demo_idx,
                'value': count,
                'genre': genre,
                'demographic': demo,
                'count': count,
                'avg_rating': round(avg_rating, 2),
                'intensity': count / max(1, max([sum(1 for w in webtoons_data if g in w['tags']) for g in genres]))
            })
    
    return heatmap_data

@app.get("/api/analysis/heatmap")
async def get_heatmap_analysis():
    """íˆíŠ¸ë§µ ë¶„ì„ ë°ì´í„° ë°˜í™˜ (í•œêµ­ì–´ ê°œì„ )"""
    try:
        webtoons_data = load_webtoon_data()
        heatmap_data = generate_heatmap_data(webtoons_data)
        
        return {
            "success": True, 
            "data": heatmap_data,
            "metadata": {
                "genres": ['ë¡œë§¨ìŠ¤', 'ì•¡ì…˜', 'íŒíƒ€ì§€', 'ë“œë¼ë§ˆ', 'ë¬´í˜‘', 'ì¼ìƒ', 'ìŠ¤ë¦´ëŸ¬'],
                "demographics": ['ë‚¨ì„±-10ëŒ€', 'ë‚¨ì„±-20ëŒ€', 'ë‚¨ì„±-30ëŒ€', 'ì—¬ì„±-10ëŒ€', 'ì—¬ì„±-20ëŒ€', 'ì—¬ì„±-30ëŒ€'],
                "total_combinations": len(heatmap_data)
            }
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"íˆíŠ¸ë§µ ë¶„ì„ ì‹¤íŒ¨: {str(e)}")

def analyze_tag_connectivity(webtoons_data, min_correlation=0.15):
    """íƒœê·¸ë³„ ì—°ê²°ì„± ë¶„ì„ - ê° íƒœê·¸ê°€ ëª‡ ê°œì˜ ë‹¤ë¥¸ íƒœê·¸ì™€ ì—°ê²°ë˜ì–´ ìˆëŠ”ì§€ ë¶„ì„"""
    print("ğŸ•¸ï¸ íƒœê·¸ ì—°ê²°ì„± ë¶„ì„ ì‹œì‘...")
    
    # íƒœê·¸ ë§¤íŠ¸ë¦­ìŠ¤ ë° ìƒê´€ê´€ê³„ ê³„ì‚°
    tag_matrix, frequent_tags, tag_counts = create_tag_matrix(webtoons_data)
    _, correlations = calculate_tag_correlations(tag_matrix, frequent_tags)
    
    # ê° íƒœê·¸ë³„ ì—°ê²°ëœ íƒœê·¸ë“¤ê³¼ ì—°ê²° ê°•ë„ ê³„ì‚°
    tag_connections = {}
    
    for tag in frequent_tags:
        connected_tags = []
        
        # ì´ íƒœê·¸ì™€ ì—°ê²°ëœ ëª¨ë“  íƒœê·¸ë“¤ ì°¾ê¸°
        for corr in correlations:
            if corr['correlation'] >= min_correlation:
                if corr['tag1'] == tag:
                    connected_tags.append({
                        'connected_tag': corr['tag2'],
                        'correlation': round(corr['correlation'], 3),
                        'co_occurrence': round(corr['co_occurrence'], 1)
                    })
                elif corr['tag2'] == tag:
                    connected_tags.append({
                        'connected_tag': corr['tag1'], 
                        'correlation': round(corr['correlation'], 3),
                        'co_occurrence': round(corr['co_occurrence'], 1)
                    })
        
        # ì—°ê²° ê°•ë„ìˆœìœ¼ë¡œ ì •ë ¬
        connected_tags.sort(key=lambda x: x['correlation'], reverse=True)
        
        tag_connections[tag] = {
            'tag': tag,
            'connection_count': len(connected_tags),
            'connected_tags': connected_tags,
            'frequency': tag_counts.get(tag, 0),
            'avg_correlation': round(np.mean([ct['correlation'] for ct in connected_tags]), 3) if connected_tags else 0,
            'category': get_korean_tag_category(tag)
        }
    
    # ì—°ê²°ì„±ì´ ë†’ì€ ìˆœìœ¼ë¡œ ì •ë ¬
    sorted_connectivity = sorted(
        tag_connections.values(),
        key=lambda x: (x['connection_count'], x['avg_correlation']),
        reverse=True
    )
    
    return sorted_connectivity

@app.get("/api/analysis/tag-connectivity")
async def get_tag_connectivity(
    min_correlation: Optional[float] = Query(0.15, description="ìµœì†Œ ìƒê´€ê³„ìˆ˜"),
    top_n: Optional[int] = Query(15, description="ìƒìœ„ Nê°œ íƒœê·¸")
):
    """íƒœê·¸ë³„ ì—°ê²°ì„± ë¶„ì„ - ê° íƒœê·¸ê°€ ëª‡ ê°œì˜ ë‹¤ë¥¸ íƒœê·¸ì™€ ì—°ê²°ë˜ì–´ ìˆëŠ”ì§€"""
    try:
        webtoons_data = load_webtoon_data()
        connectivity_data = analyze_tag_connectivity(webtoons_data, min_correlation)
        
        # ìƒìœ„ Nê°œë§Œ ì„ íƒ
        top_connectivity = connectivity_data[:top_n]
        
        # ìš”ì•½ í†µê³„
        summary = {
            "total_analyzed_tags": len(connectivity_data),
            "min_correlation_threshold": min_correlation,
            "most_connected_tag": connectivity_data[0]['tag'] if connectivity_data else None,
            "max_connections": connectivity_data[0]['connection_count'] if connectivity_data else 0,
            "avg_connections": round(np.mean([t['connection_count'] for t in connectivity_data]), 1) if connectivity_data else 0
        }
        
        return {
            "success": True,
            "data": {
                "top_connected_tags": top_connectivity,
                "summary": summary,
                "analysis_info": {
                    "description": "ê° íƒœê·¸ê°€ ë‹¤ë¥¸ íƒœê·¸ë“¤ê³¼ ì–¼ë§ˆë‚˜ ê°•í•˜ê²Œ ì—°ê²°ë˜ì–´ ìˆëŠ”ì§€ ë¶„ì„",
                    "correlation_method": "cosine_similarity",
                    "weight_factors": ["rating", "interest_count"],
                    "min_tag_frequency": 3
                }
            }
        }
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"íƒœê·¸ ì—°ê²°ì„± ë¶„ì„ ì‹¤íŒ¨: {str(e)}")

@app.get("/api/analysis/insights")
async def get_insights():
    """ë°ì´í„° ê¸°ë°˜ ì¸ì‚¬ì´íŠ¸ ì œê³µ"""
    try:
        webtoons_data = load_webtoon_data()
        
        # íƒœê·¸ íŠ¸ë Œë“œ ë¶„ì„
        all_tags = []
        for w in webtoons_data:
            all_tags.extend(w['tags'])
        tag_frequency = Counter(all_tags)
        
        # ì„±ë³„ë³„ ì„ í˜¸ íƒœê·¸
        male_tags = []
        female_tags = []
        for w in webtoons_data:
            if w['gender'] == 'ë‚¨ì„±':
                male_tags.extend(w['tags'])
            else:
                female_tags.extend(w['tags'])
        
        male_preferences = Counter(male_tags).most_common(10)
        female_preferences = Counter(female_tags).most_common(10)
        
        # ê³ í‰ì  íƒœê·¸ ë¶„ì„
        high_rated_tags = defaultdict(list)
        for w in webtoons_data:
            if w['rating'] >= 9.5:
                for tag in w['tags']:
                    high_rated_tags[tag].append(w['rating'])
        
        quality_tags = {
            tag: round(np.mean(ratings), 2) 
            for tag, ratings in high_rated_tags.items() 
            if len(ratings) >= 3
        }
        quality_tags = sorted(quality_tags.items(), key=lambda x: x[1], reverse=True)[:10]
        
        return {
            "success": True,
            "data": {
                "trending_tags": tag_frequency.most_common(10),
                "male_preferences": male_preferences,
                "female_preferences": female_preferences,
                "quality_indicators": quality_tags,
                "insights": {
                    "most_popular_genre": tag_frequency.most_common(1)[0][0],
                    "gender_difference": len(set(dict(male_preferences[:5]).keys()) - set(dict(female_preferences[:5]).keys())),
                    "quality_vs_popularity": "í‰ì ê³¼ ì¸ê¸°ë„ì˜ ìƒê´€ê´€ê³„ ë¶„ì„ ê²°ê³¼"
                }
            }
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ì¸ì‚¬ì´íŠ¸ ë¶„ì„ ì‹¤íŒ¨: {str(e)}")

@app.get("/health")
async def health_check():
    """í—¬ìŠ¤ ì²´í¬ ì—”ë“œí¬ì¸íŠ¸"""
    return {
        "status": "healthy",
        "timestamp": datetime.now().isoformat(),
        "version": "1.0.0",
        "features": ["korean_tags", "advanced_network", "weighted_similarity"]
    }

if __name__ == "__main__":
    port = int(os.environ.get("PORT", 8000))
    uvicorn.run("main:app", host="0.0.0.0", port=port, reload=False)